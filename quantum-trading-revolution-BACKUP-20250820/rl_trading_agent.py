"""
ğŸš€ REINFORCEMENT LEARNING TRADING AGENT
âœ… IntÃ©gration PPO pour optimisation automatique des dÃ©cisions
ğŸ¯ Objectif: +10-15% optimisation des signaux trading
"""

import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pandas as pd
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - [%(name)s] %(message)s')
logger = logging.getLogger("RL_TRADING_AGENT")

# VÃ©rifier si stable-baselines3 est disponible
try:
    from stable_baselines3 import PPO
    PPO_AVAILABLE = True
    logger.info("âœ… Stable-Baselines3 disponible")
except ImportError:
    PPO_AVAILABLE = False
    logger.warning("âš ï¸ Stable-Baselines3 non disponible - Mode simulation activÃ©")

class TradingEnvironment(gym.Env):
    """Environnement de trading pour Reinforcement Learning"""
    
    def __init__(self, data: pd.DataFrame, initial_balance: float = 100000):
        super(TradingEnvironment, self).__init__()
        
        # ACTIONS: 0=HOLD, 1=BUY, 2=SELL
        self.action_space = spaces.Discrete(3)
        
        # OBSERVATIONS: Prix, RSI, MACD, Volume, Position actuelle, Balance, Profit, Drawdown
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(8,), dtype=np.float32
        )
        
        self.data = data
        self.initial_balance = initial_balance
        self.reset()
        
        logger.info(f"ğŸš€ Environnement RL crÃ©Ã© avec {len(data)} pÃ©riodes")
        logger.info(f"   Balance initiale: ${initial_balance:,.2f}")
    
    def reset(self, seed=None, options=None):
        """RÃ©initialiser l'environnement"""
        super().reset(seed=seed)
        
        self.current_step = 0
        self.balance = self.initial_balance
        self.position = 0  # 0=pas de position, 1=long, -1=short
        self.entry_price = 0
        self.total_trades = 0
        self.winning_trades = 0
        self.max_balance = self.initial_balance
        self.min_balance = self.initial_balance
        
        return self._get_observation(), {}
    
    def _get_observation(self):
        """Retourner l'observation actuelle"""
        if self.current_step >= len(self.data):
            return np.zeros(8, dtype=np.float32)
        
        current_data = self.data.iloc[self.current_step]
        
        # RETOURNER: [prix, rsi, macd, volume, position, balance, profit, drawdown]
        obs = np.array([
            current_data.get('close', 100.0),
            current_data.get('rsi', 50.0),
            current_data.get('macd', 0.0),
            current_data.get('volume', 1000000),
            self.position,
            self.balance,
            self._calculate_profit(),
            self._calculate_drawdown()
        ], dtype=np.float32)
        
        return obs
    
    def _calculate_profit(self):
        """Calculer le profit actuel"""
        if self.position == 0:
            return 0.0
        
        current_price = self.data.iloc[self.current_step]['close']
        if self.position == 1:  # Long position
            return current_price - self.entry_price
        else:  # Short position
            return self.entry_price - current_price
    
    def _calculate_drawdown(self):
        """Calculer le drawdown actuel"""
        if self.balance > self.max_balance:
            self.max_balance = self.balance
        
        if self.balance < self.min_balance:
            self.min_balance = self.balance
        
        if self.max_balance == 0:
            return 0.0
        
        return (self.max_balance - self.balance) / self.max_balance
    
    def step(self, action):
        """ExÃ©cuter une action et retourner le nouvel Ã©tat"""
        if self.current_step >= len(self.data) - 1:
            return self._get_observation(), 0, True, {}
        
        current_price = self.data.iloc[self.current_step]['close']
        reward = 0
        info = {}
        
        # ExÃ©cuter l'action
        if action == 1 and self.position == 0:  # BUY
            self.position = 1
            self.entry_price = current_price
            self.total_trades += 1
            info['action'] = 'BUY'
            info['entry_price'] = current_price
            
        elif action == 2 and self.position == 1:  # SELL
            profit = current_price - self.entry_price
            self.balance += profit
            reward = profit
            
            if profit > 0:
                self.winning_trades += 1
            
            self.position = 0
            self.entry_price = 0
            info['action'] = 'SELL'
            info['profit'] = profit
            info['win_rate'] = self.winning_trades / self.total_trades if self.total_trades > 0 else 0
        
        # Mettre Ã  jour le step
        self.current_step += 1
        done = self.current_step >= len(self.data) - 1
        
        # Calculer le reward final si terminÃ©
        if done:
            final_profit = self.balance - self.initial_balance
            reward += final_profit * 0.1  # Bonus final
            info['final_balance'] = self.balance
            info['total_return'] = (self.balance / self.initial_balance - 1) * 100
            info['win_rate'] = self.winning_trades / self.total_trades if self.total_trades > 0 else 0
        
        return self._get_observation(), reward, done, False, info

class RLTrader:
    """Trader utilisant le Reinforcement Learning"""
    
    def __init__(self, model_path: Optional[str] = None):
        self.model = None
        self.environment = None
        self.model_path = model_path
        self.is_trained = False
        
        logger.info("ğŸ¤– RL Trader initialisÃ©")
    
    def train_model(self, historical_data: pd.DataFrame, total_timesteps: int = 10000):
        """EntraÃ®ner le modÃ¨le RL sur donnÃ©es historiques"""
        if not PPO_AVAILABLE:
            logger.warning("âš ï¸ PPO non disponible - Mode simulation activÃ©")
            self.is_trained = True
            return
        
        try:
            logger.info(f"ğŸš€ DÃ©but entraÃ®nement RL sur {len(historical_data)} pÃ©riodes")
            logger.info(f"   Timesteps: {total_timesteps:,}")
            
            # CrÃ©er l'environnement
            self.environment = TradingEnvironment(historical_data)
            
            # CrÃ©er et entraÃ®ner le modÃ¨le
            self.model = PPO("MlpPolicy", self.environment, verbose=1)
            self.model.learn(total_timesteps=total_timesteps)
            
            # Sauvegarder le modÃ¨le
            if self.model_path:
                self.model.save(self.model_path)
                logger.info(f"ğŸ’¾ ModÃ¨le sauvegardÃ©: {self.model_path}")
            
            self.is_trained = True
            logger.info("âœ… EntraÃ®nement RL terminÃ© avec succÃ¨s")
            
        except Exception as e:
            logger.error(f"âŒ Erreur entraÃ®nement RL: {e}")
            self.is_trained = False
    
    def load_model(self, model_path: str):
        """Charger un modÃ¨le prÃ©-entraÃ®nÃ©"""
        if not PPO_AVAILABLE:
            logger.warning("âš ï¸ PPO non disponible - Impossible de charger le modÃ¨le")
            return False
        
        try:
            self.model = PPO.load(model_path)
            self.model_path = model_path
            self.is_trained = True
            logger.info(f"âœ… ModÃ¨le chargÃ©: {model_path}")
            return True
        except Exception as e:
            logger.error(f"âŒ Erreur chargement modÃ¨le: {e}")
            return False
    
    def get_rl_decision(self, current_state: List[float]) -> int:
        """Obtenir dÃ©cision du modÃ¨le RL"""
        if not self.is_trained or self.model is None:
            return 0  # HOLD par dÃ©faut
        
        try:
            obs = np.array(current_state, dtype=np.float32)
            action, _ = self.model.predict(obs)
            return int(action)
        except Exception as e:
            logger.error(f"âŒ Erreur prÃ©diction RL: {e}")
            return 0  # HOLD en cas d'erreur
    
    def evaluate_model(self, test_data: pd.DataFrame) -> Dict:
        """Ã‰valuer la performance du modÃ¨le sur donnÃ©es de test"""
        if not self.is_trained or self.model is None:
            return {'error': 'ModÃ¨le non entraÃ®nÃ©'}
        
        try:
            test_env = TradingEnvironment(test_data)
            obs = test_env.reset()
            done = False
            total_reward = 0
            trades = []
            
            while not done:
                action = self.get_rl_decision(obs)
                obs, reward, done, truncated, info = test_env.step(action)
                total_reward += reward
                
                if 'action' in info:
                    trades.append(info)
            
            # Calculer mÃ©triques
            final_balance = test_env.balance
            total_return = (final_balance / test_env.initial_balance - 1) * 100
            win_rate = test_env.winning_trades / test_env.total_trades if test_env.total_trades > 0 else 0
            
            results = {
                'total_reward': total_reward,
                'final_balance': final_balance,
                'total_return_pct': total_return,
                'total_trades': test_env.total_trades,
                'winning_trades': test_env.winning_trades,
                'win_rate': win_rate,
                'max_drawdown': test_env._calculate_drawdown()
            }
            
            logger.info(f"ğŸ“Š Ã‰valuation RL - Retour: {total_return:.2f}%, Win Rate: {win_rate:.1%}")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Erreur Ã©valuation RL: {e}")
            return {'error': str(e)}

def create_sample_data(n_periods: int = 1000) -> pd.DataFrame:
    """CrÃ©er des donnÃ©es d'exemple pour test"""
    np.random.seed(42)
    
    # Prix simulÃ©s avec tendance
    base_price = 100.0
    prices = [base_price]
    
    for i in range(1, n_periods):
        # Tendance + bruit
        trend = 0.0001 * i  # Tendance lÃ©gÃ¨rement haussiÃ¨re
        noise = np.random.normal(0, 0.02)  # 2% volatilitÃ©
        new_price = prices[-1] * (1 + trend + noise)
        prices.append(max(new_price, 1.0))  # Prix minimum $1
    
    # CrÃ©er DataFrame
    df = pd.DataFrame({
        'open': prices,
        'high': [p * (1 + abs(np.random.normal(0, 0.01))) for p in prices],
        'low': [p * (1 - abs(np.random.normal(0, 0.01))) for p in prices],
        'close': prices,
        'volume': np.random.randint(100000, 1000000, n_periods),
        'rsi': np.random.uniform(20, 80, n_periods),
        'macd': np.random.uniform(-2, 2, n_periods)
    })
    
    return df

def main():
    """Test du systÃ¨me RL"""
    print("ğŸš€" + "="*80 + "ğŸš€")
    print("   ğŸ”¥ REINFORCEMENT LEARNING TRADING AGENT")
    print("="*84)
    print("   âœ… Environnement de trading gym")
    print("   âœ… ModÃ¨le PPO (si disponible)")
    print("   ğŸ¯ Objectif: +10-15% optimisation trading")
    print("ğŸš€" + "="*80 + "ğŸš€")
    
    # CrÃ©er donnÃ©es d'exemple
    print("\nğŸ“Š CrÃ©ation donnÃ©es d'exemple...")
    sample_data = create_sample_data(500)
    print(f"   âœ… {len(sample_data)} pÃ©riodes crÃ©Ã©es")
    
    # Diviser en train/test
    split_idx = int(len(sample_data) * 0.8)
    train_data = sample_data[:split_idx]
    test_data = sample_data[split_idx:]
    
    print(f"   ğŸ“ˆ EntraÃ®nement: {len(train_data)} pÃ©riodes")
    print(f"   ğŸ§ª Test: {len(test_data)} pÃ©riodes")
    
    # Initialiser le trader RL
    rl_trader = RLTrader()
    
    # EntraÃ®ner le modÃ¨le
    print("\nğŸ¤– EntraÃ®nement du modÃ¨le RL...")
    rl_trader.train_model(train_data, total_timesteps=5000)
    
    if rl_trader.is_trained:
        # Ã‰valuer le modÃ¨le
        print("\nğŸ“Š Ã‰valuation du modÃ¨le...")
        results = rl_trader.evaluate_model(test_data)
        
        if 'error' not in results:
            print(f"   ğŸ¯ Retour total: {results['total_return_pct']:.2f}%")
            print(f"   ğŸ“ˆ Trades: {results['total_trades']}")
            print(f"   âœ… Win Rate: {results['win_rate']:.1%}")
            print(f"   ğŸ’° Balance finale: ${results['final_balance']:,.2f}")
        else:
            print(f"   âŒ Erreur: {results['error']}")
    else:
        print("   âš ï¸ ModÃ¨le non entraÃ®nÃ© - Mode simulation")
    
    print("\nâœ… Test RL terminÃ©!")

if __name__ == "__main__":
    main()
