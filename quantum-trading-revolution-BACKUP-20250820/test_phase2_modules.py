# FILE: test_phase2_modules.py
"""
AGENT QUANTUM TRADING - TESTS DE VALIDATION PHASE 2
Script de test complet pour tous les modules Phase 2
Validation: Model Monitoring, Online Learning, Multi-Agent, Regime Detection, Ultra-Low Latency
"""

import time
import numpy as np
import pandas as pd
import warnings
from typing import Dict, List, Any
import json
import traceback

warnings.filterwarnings('ignore')

# Import des modules Phase 2
try:
    from model_monitoring_system import ModelMonitoringSystem
    from online_learning_framework import OnlineLearningFramework
    from multi_agent_architecture import MasterCoordinatorAgent, RedisCommunicationLayer
    from regime_detection_hybrid import RegimeDetectionEnsemble
    from ultra_low_latency_engine import UltraLowLatencyEngine
    print("OK - Tous les modules Phase 2 importÃ©s pour tests")
except ImportError as e:
    print(f"âŒ Erreur import module: {e}")
    exit(1)

class Phase2ModuleTester:
    """Testeur complet des modules Phase 2"""
    
    def __init__(self):
        self.test_results = {}
        self.test_data = self._generate_test_data()
        print("ğŸ§ª Phase 2 Module Tester initialisÃ©")
    
    def _generate_test_data(self) -> Dict:
        """GÃ©nÃ©ration de donnÃ©es de test rÃ©alistes"""
        # DonnÃ©es de marchÃ© simulÃ©es
        np.random.seed(42)  # ReproductibilitÃ©
        
        # Prix avec tendance et volatilitÃ©
        n_points = 1000
        trend = np.linspace(100, 120, n_points)
        noise = np.random.randn(n_points) * 2
        prices = trend + noise
        
        # Volumes
        volumes = np.random.randint(1000, 10000, n_points)
        
        # Features techniques
        returns = np.diff(prices) / prices[:-1]
        volatility = np.array([np.std(returns[max(0, i-20):i+1]) for i in range(len(returns))])
        
        return {
            'prices': prices,
            'volumes': volumes,
            'returns': returns,
            'volatility': volatility,
            'timestamps': np.arange(n_points)
        }
    
    def test_model_monitoring_system(self) -> Dict:
        """Test du systÃ¨me de monitoring de modÃ¨les"""
        print("\nğŸ” TEST: Model Monitoring System")
        print("-" * 40)
        
        try:
            # Initialisation
            monitoring_system = ModelMonitoringSystem()
            
            # Test 1: Surveillance des donnÃ©es
            print("   ğŸ“Š Test surveillance donnÃ©es...")
            data_quality = monitoring_system.check_data_quality(self.test_data['prices'])
            print(f"      âœ… QualitÃ© donnÃ©es: {data_quality.get('overall_score', 'N/A')}")
            
            # Test 2: DÃ©tection de dÃ©rive
            print("   ğŸš¨ Test dÃ©tection dÃ©rive...")
            drift_result = monitoring_system.detect_data_drift(
                self.test_data['prices'][:500], 
                self.test_data['prices'][500:]
            )
            print(f"      âœ… DÃ©rive dÃ©tectÃ©e: {drift_result.get('drift_detected', 'N/A')}")
            
            # Test 3: Monitoring des performances
            print("   ğŸ“ˆ Test monitoring performances...")
            performance = monitoring_system.monitor_model_performance(
                predictions=np.random.rand(100),
                actuals=np.random.rand(100)
            )
            print(f"      âœ… Performance: {performance.get('accuracy', 'N/A'):.3f}")
            
            # Test 4: SystÃ¨me d'alertes
            print("   ğŸš¨ Test systÃ¨me alertes...")
            alert_result = monitoring_system.trigger_alert('test_alert', 'Test alert message')
            print(f"      âœ… Alerte dÃ©clenchÃ©e: {alert_result}")
            
            # Test 5: SantÃ© du systÃ¨me
            health = monitoring_system.get_system_health()
            print(f"      âœ… SantÃ© systÃ¨me: {health.get('overall_health', 'N/A')}")
            
            self.test_results['model_monitoring'] = {
                'status': 'PASSED',
                'tests': 5,
                'passed': 5,
                'failed': 0,
                'details': {
                    'data_quality': data_quality,
                    'drift_detection': drift_result,
                    'performance_monitoring': performance,
                    'alert_system': alert_result,
                    'system_health': health
                }
            }
            
            print("   OK - Model Monitoring System: TOUS LES TESTS RÃ‰USSIS")
            return self.test_results['model_monitoring']
            
        except Exception as e:
            print(f"   âŒ Erreur test Model Monitoring: {e}")
            self.test_results['model_monitoring'] = {
                'status': 'FAILED',
                'error': str(e),
                'traceback': traceback.format_exc()
            }
            return self.test_results['model_monitoring']
    
    def test_online_learning_framework(self) -> Dict:
        """Test du framework d'apprentissage en ligne"""
        print("\nğŸ” TEST: Online Learning Framework")
        print("-" * 40)
        
        try:
            # Initialisation
            framework = OnlineLearningFramework()
            
            # Test 1: Initialisation des modÃ¨les
            print("   ğŸ¤– Test initialisation modÃ¨les...")
            models = framework.get_active_models()
            print(f"      âœ… ModÃ¨les actifs: {len(models)}")
            
            # Test 2: EntraÃ®nement incrÃ©mental
            print("   ğŸ“š Test entraÃ®nement incrÃ©mental...")
            training_data = np.random.rand(100, 5)
            training_labels = np.random.randint(0, 2, 100)
            
            for i in range(0, 100, 10):
                batch_data = training_data[i:i+10]
                batch_labels = training_labels[i:i+10]
                framework.train_incremental(batch_data, batch_labels)
            
            print("      âœ… EntraÃ®nement incrÃ©mental rÃ©ussi")
            
            # Test 3: PrÃ©dictions
            print("   ğŸ”® Test prÃ©dictions...")
            test_data = np.random.rand(20, 5)
            predictions = framework.predict(test_data)
            print(f"      âœ… PrÃ©dictions gÃ©nÃ©rÃ©es: {len(predictions)}")
            
            # Test 4: DÃ©tection de concept drift
            print("   ğŸŒŠ Test dÃ©tection concept drift...")
            drift_detected = framework.detect_concept_drift(test_data)
            print(f"      âœ… Concept drift: {drift_detected}")
            
            # Test 5: MÃ©triques de performance
            performance = framework.get_performance_metrics()
            print(f"      âœ… MÃ©triques: {len(performance)} indicateurs")
            
            self.test_results['online_learning'] = {
                'status': 'PASSED',
                'tests': 5,
                'passed': 5,
                'failed': 0,
                'details': {
                    'active_models': len(models),
                    'training_success': True,
                    'predictions_count': len(predictions),
                    'concept_drift': drift_detected,
                    'performance_metrics': performance
                }
            }
            
            print("   OK - Online Learning Framework: TOUS LES TESTS RÃ‰USSIS")
            return self.test_results['online_learning']
            
        except Exception as e:
            print(f"   âŒ Erreur test Online Learning: {e}")
            self.test_results['online_learning'] = {
                'status': 'FAILED',
                'error': str(e),
                'traceback': traceback.format_exc()
            }
            return self.test_results['online_learning']
    
    def test_multi_agent_architecture(self) -> Dict:
        """Test de l'architecture multi-agents"""
        print("\nğŸ” TEST: Multi-Agent Architecture")
        print("-" * 40)
        
        try:
            # Initialisation
            communication_layer = RedisCommunicationLayer()
            coordinator = MasterCoordinatorAgent(communication_layer)
            
            # Test 1: Initialisation des agents
            print("   ğŸ¤– Test initialisation agents...")
            active_agents = coordinator.get_active_agents()
            print(f"      âœ… Agents actifs: {len(active_agents)}")
            
            # Test 2: Communication inter-agents
            print("   ğŸ“¡ Test communication inter-agents...")
            message_sent = communication_layer.send_message(
                'test_channel', 
                {'type': 'test', 'data': 'test_message'}
            )
            print(f"      âœ… Message envoyÃ©: {message_sent}")
            
            # Test 3: Coordination des agents
            print("   ğŸ¯ Test coordination agents...")
            coordination_result = coordinator.coordinate_agents()
            print(f"      âœ… Coordination: {coordination_result}")
            
            # Test 4: Gestion des urgences
            print("   ğŸš¨ Test gestion urgences...")
            emergency_result = coordinator.handle_emergency('test_emergency')
            print(f"      âœ… Urgence gÃ©rÃ©e: {emergency_result}")
            
            # Test 5: MÃ©triques de performance
            performance = coordinator.get_system_performance()
            print(f"      âœ… Performance systÃ¨me: {len(performance)} mÃ©triques")
            
            self.test_results['multi_agent'] = {
                'status': 'PASSED',
                'tests': 5,
                'passed': 5,
                'failed': 0,
                'details': {
                    'active_agents': len(active_agents),
                    'communication_success': message_sent,
                    'coordination_result': coordination_result,
                    'emergency_handling': emergency_result,
                    'system_performance': performance
                }
            }
            
            print("   OK - Multi-Agent Architecture: TOUS LES TESTS RÃ‰USSIS")
            return self.test_results['multi_agent']
            
        except Exception as e:
            print(f"   âŒ Erreur test Multi-Agent: {e}")
            self.test_results['multi_agent'] = {
                'status': 'FAILED',
                'error': str(e),
                'traceback': traceback.format_exc()
            }
            return self.test_results['multi_agent']
    
    def test_regime_detection_hybrid(self) -> Dict:
        """Test du systÃ¨me de dÃ©tection de rÃ©gimes hybrides"""
        print("\nğŸ” TEST: Regime Detection Hybrid")
        print("-" * 40)
        
        try:
            # Initialisation
            regime_ensemble = RegimeDetectionEnsemble()
            
            # Test 1: Initialisation des dÃ©tecteurs
            print("   ğŸ” Test initialisation dÃ©tecteurs...")
            detectors = regime_ensemble.detectors
            print(f"      âœ… DÃ©tecteurs initialisÃ©s: {len(detectors)}")
            
            # Test 2: EntraÃ®nement de l'ensemble
            print("   ğŸ¯ Test entraÃ®nement ensemble...")
            training_success = regime_ensemble.train_ensemble(
                self.test_data['prices']
            )
            print(f"      âœ… EntraÃ®nement: {training_success}")
            
            # Test 3: DÃ©tection de rÃ©gime
            print("   ğŸŒŠ Test dÃ©tection rÃ©gime...")
            regime_result = regime_ensemble.detect_regime_ensemble(
                self.test_data['prices'][-100:]
            )
            print(f"      âœ… RÃ©gime dÃ©tectÃ©: {regime_result.get('ensemble_regime', 'N/A')}")
            
            # Test 4: Performance de l'ensemble
            print("   ğŸ“Š Test performance ensemble...")
            performance = regime_ensemble.get_ensemble_performance()
            print(f"      âœ… Performance: {performance.get('total_detections', 0)} dÃ©tections")
            
            # Test 5: Mise Ã  jour des poids
            print("   âš–ï¸ Test mise Ã  jour poids...")
            regime_ensemble.update_ensemble_weights(performance)
            print("      âœ… Poids mis Ã  jour")
            
            self.test_results['regime_detection'] = {
                'status': 'PASSED',
                'tests': 5,
                'passed': 5,
                'failed': 0,
                'details': {
                    'detectors_count': len(detectors),
                    'training_success': training_success,
                    'regime_detected': regime_result.get('ensemble_regime'),
                    'performance_metrics': performance,
                    'weights_updated': True
                }
            }
            
            print("   OK - Regime Detection Hybrid: TOUS LES TESTS RÃ‰USSIS")
            return self.test_results['regime_detection']
            
        except Exception as e:
            print(f"   âŒ Erreur test Regime Detection: {e}")
            self.test_results['regime_detection'] = {
                'status': 'FAILED',
                'error': str(e),
                'traceback': traceback.format_exc()
            }
            return self.test_results['regime_detection']
    
    def test_ultra_low_latency_engine(self) -> Dict:
        """Test du moteur de latence ultra-faible"""
        print("\nğŸ” TEST: Ultra-Low Latency Engine")
        print("-" * 40)
        
        try:
            # Initialisation
            engine = UltraLowLatencyEngine()
            
            # Test 1: Initialisation des composants
            print("   âš¡ Test initialisation composants...")
            components = [
                engine.event_bus,
                engine.tsdb,
                engine.cache,
                engine.signal_computer
            ]
            print(f"      âœ… Composants initialisÃ©s: {len(components)}")
            
            # Test 2: Performance du cache
            print("   ğŸ’¾ Test performance cache...")
            for i in range(100):
                engine.cache.set(f"key_{i}", f"value_{i}")
            
            cache_stats = engine.cache.get_stats()
            print(f"      âœ… Cache: {cache_stats.get('hit_rate', 0):.2f} hit rate")
            
            # Test 3: Calcul de signaux
            print("   ğŸ§® Test calcul signaux...")
            test_data = self.test_data['prices'][:100]
            start_time = time.time()
            signals = engine.signal_computer.compute_signals_fast(test_data)
            end_time = time.time()
            
            computation_time = (end_time - start_time) * 1000  # ms
            print(f"      âœ… Signaux calculÃ©s: {len(signals)} en {computation_time:.2f}ms")
            
            # Test 4: Stockage TSDB
            print("   ğŸ—„ï¸ Test stockage TSDB...")
            for i in range(50):
                engine.tsdb.store_series('test_series', time.time(), i)
            
            tsdb_stats = engine.tsdb.get_stats()
            print(f"      âœ… TSDB: {tsdb_stats.get('total_points', 0)} points")
            
            # Test 5: MÃ©triques de performance
            print("   ğŸ“Š Test mÃ©triques performance...")
            performance = engine.get_performance_metrics()
            print(f"      âœ… MÃ©triques: {len(performance)} indicateurs")
            
            self.test_results['ultra_latency'] = {
                'status': 'PASSED',
                'tests': 5,
                'passed': 5,
                'failed': 0,
                'details': {
                    'components_initialized': len(components),
                    'cache_performance': cache_stats,
                    'signal_computation_time_ms': computation_time,
                    'tsdb_points': tsdb_stats.get('total_points', 0),
                    'performance_metrics': performance
                }
            }
            
            print("   OK - Ultra-Low Latency Engine: TOUS LES TESTS RÃ‰USSIS")
            return self.test_results['ultra_latency']
            
        except Exception as e:
            print(f"   âŒ Erreur test Ultra-Low Latency: {e}")
            self.test_results['ultra_latency'] = {
                'status': 'FAILED',
                'error': str(e),
                'traceback': traceback.format_exc()
            }
            return self.test_results['ultra_latency']
    
    def run_all_tests(self) -> Dict:
        """ExÃ©cution de tous les tests"""
        print("ğŸš€ LANCEMENT TESTS COMPLETS PHASE 2")
        print("=" * 60)
        
        start_time = time.time()
        
        # Tests individuels
        self.test_model_monitoring_system()
        self.test_online_learning_framework()
        self.test_multi_agent_architecture()
        self.test_regime_detection_hybrid()
        self.test_ultra_low_latency_engine()
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # RÃ©sumÃ© des tests
        self.generate_test_summary(total_time)
        
        return self.test_results
    
    def generate_test_summary(self, total_time: float):
        """GÃ©nÃ©ration du rÃ©sumÃ© des tests"""
        print("\nğŸ“‹ RÃ‰SUMÃ‰ DES TESTS PHASE 2")
        print("=" * 60)
        
        total_tests = 0
        passed_tests = 0
        failed_tests = 0
        
        for module_name, result in self.test_results.items():
            if result['status'] == 'PASSED':
                status_icon = "âœ…"
                passed_tests += result.get('tests', 0)
                total_tests += result.get('tests', 0)
            else:
                status_icon = "âŒ"
                failed_tests += 1
                total_tests += 1
            
            print(f"   {status_icon} {module_name}: {result['status']}")
        
        # Statistiques globales
        print(f"\nğŸ“Š STATISTIQUES GLOBALES:")
        print(f"   â€¢ Tests totaux: {total_tests}")
        print(f"   â€¢ Tests rÃ©ussis: {passed_tests}")
        print(f"   â€¢ Tests Ã©chouÃ©s: {failed_tests}")
        print(f"   â€¢ Taux de succÃ¨s: {(passed_tests/total_tests*100):.1f}%" if total_tests > 0 else "   â€¢ Taux de succÃ¨s: N/A")
        print(f"   â€¢ Temps total: {total_time:.2f}s")
        
        # Sauvegarde des rÃ©sultats
        self.save_test_results(total_time)
        
        # Conclusion
        if failed_tests == 0:
            print(f"\nğŸ‰ TOUS LES TESTS PHASE 2 RÃ‰USSIS !")
            print("   L'agent est prÃªt pour la transformation TIER 1+")
        else:
            print(f"\nâš ï¸ {failed_tests} TESTS Ã‰CHOUÃ‰S")
            print("   VÃ©rification requise avant dÃ©ploiement")
    
    def save_test_results(self, total_time: float):
        """Sauvegarde des rÃ©sultats des tests"""
        test_report = {
            'timestamp': time.time(),
            'test_duration_seconds': total_time,
            'modules_tested': list(self.test_results.keys()),
            'results': self.test_results,
            'summary': {
                'total_tests': sum(r.get('tests', 0) for r in self.test_results.values() if r['status'] == 'PASSED'),
                'passed_tests': sum(r.get('tests', 0) for r in self.test_results.values() if r['status'] == 'PASSED'),
                'failed_modules': sum(1 for r in self.test_results.values() if r['status'] == 'FAILED')
            }
        }
        
        try:
            with open('phase2_test_results.json', 'w') as f:
                json.dump(test_report, f, indent=2, default=str)
            print("   ğŸ’¾ RÃ©sultats sauvegardÃ©s: phase2_test_results.json")
        except Exception as e:
            print(f"   âš ï¸ Erreur sauvegarde rÃ©sultats: {e}")

def run_phase2_tests():
    """Fonction principale pour exÃ©cuter les tests Phase 2"""
    print("ğŸ§ª LANCEMENT TESTS DE VALIDATION PHASE 2")
    print("=" * 70)
    
    # Initialisation du testeur
    tester = Phase2ModuleTester()
    
    # ExÃ©cution de tous les tests
    results = tester.run_all_tests()
    
    return results

if __name__ == "__main__":
    # Lancement des tests
    test_results = run_phase2_tests()
    
    # Affichage des rÃ©sultats finaux
    if all(r['status'] == 'PASSED' for r in test_results.values()):
        print("\nâœ… VALIDATION PHASE 2 COMPLÃˆTE - Agent certifiÃ© TIER 1+ !")
    else:
        print("\nâŒ VALIDATION PHASE 2 INCOMPLÃˆTE - VÃ©rification requise")
